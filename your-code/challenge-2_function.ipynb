{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 2: Machine Learning Pipeline\n",
    "\n",
    "In a new Jupyter Notebook, combine all your codes into a function (or a class). Your new function will execute the complete machine learning pipeline job by receiving the dataset location and output the classifier. This will allow you to use your function to predict the sentiment of any tweet in real time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    s = s.lower()\n",
    "    # URL pattern found here https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url\n",
    "    url_pattern = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    s = re.sub(url_pattern,'', s)\n",
    "    s = re.sub('@','',s)\n",
    "    s = re.sub('\\W',' ',s)\n",
    "    s = re.sub('\\d','',s)\n",
    "    s = s.rstrip()\n",
    "    return s\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s) \n",
    "\n",
    "def stem_and_lemmatize(l):\n",
    "    ps = PorterStemmer()\n",
    "    l_stemmed = [ps.stem(w) for w in l]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    l_lemmatized = [lemmatizer.lemmatize(word) for word in l_stemmed]\n",
    "    return l_lemmatized\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    return [word for word in l if not word in stopwords.words()]\n",
    "\n",
    "# Let's combine them all in one function\n",
    "\n",
    "def full_clean_up(s):\n",
    "    return remove_stopwords(stem_and_lemmatize(tokenize(clean_up(s))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_machine_learning(data):\n",
    "    # Process the tweet text\n",
    "    data['text_processed'] = data['text'].apply(full_clean_up)\n",
    "    \n",
    "    # Bag of Words\n",
    "    all_words = []\n",
    "    for word in data['text_processed']:\n",
    "        for w in word:\n",
    "            all_words.append(w)\n",
    "    \n",
    "    all_words = nltk.FreqDist(all_words)\n",
    "    top_words = [ x[0] for x in all_words.most_common(500)]\n",
    "    \n",
    "    # Create features\n",
    "    feature_set = []\n",
    "    for i in range(len(data['text_processed'])):\n",
    "        if list(data['target'])[i] == 4:\n",
    "            sentiment = 'positive'\n",
    "        else:\n",
    "            sentiment = 'negative'\n",
    "        features = {}\n",
    "        for w in top_words:\n",
    "            features[w] = (w in list(data['text_processed'])[i])\n",
    "        feature_set.append((features, sentiment))\n",
    "    \n",
    "#     list(data.apply(find_features, axis=1))\n",
    "    feature_set_train = feature_set[:round(len(feature_set)/2)]\n",
    "    feature_set_test = feature_set[round(len(feature_set)/2):]\n",
    "    \n",
    "    # Create and train the classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(feature_set_train)\n",
    "    \n",
    "    # I'll return the classifier to be able to use it\n",
    "    # but we also need the top_words list to be able to create the features of any new tweet\n",
    "    return (classifier, top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function on the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('C:/Users/ebour/Documents/()_Ironhack_DA_Bootcamp/()_Labs/sentiment140.csv').drop('flag', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll only take a small sample to test if it works\n",
    "tweets_sample = tweets.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_output = tweet_machine_learning(tweets_sample)\n",
    "classifier = classifier_output[0]\n",
    "top_words = classifier_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll write a function to create the features of any new tweet that we would want to classify\n",
    "\n",
    "def tweet_features(tweet):\n",
    "    # Create clean list of words\n",
    "    words = full_clean_up(tweet)\n",
    "    \n",
    "    # create empty features dictionnary\n",
    "    features = {}\n",
    "    \n",
    "    # iterate through the top_word list and check if each of the words is in our tweet or not\n",
    "    for w in top_words:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    # return the features dictionnary\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of the different program elements:\n",
    "\n",
    "tweet_test = \"This is not a real tweet\"\n",
    "\n",
    "# new tweet features\n",
    "test_features = tweet_features(tweet_test)\n",
    "\n",
    "# new tweet classification based on features\n",
    "classifier.classify(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll write a function that combines both operations done above, cleaning/tokening and classification\n",
    "\n",
    "def classify_tweet(tweet):\n",
    "    tweet_feat = tweet_features(tweet)\n",
    "    classification = classifier.classify(tweet_feat)\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of the function on a new tweet (should end up being positive)\n",
    "pos_tweet = \"this is awesome\"\n",
    "classify_tweet(pos_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of the function on a new tweet (should end up being negative)\n",
    "neg_tweet = \"this is sad\"\n",
    "classify_tweet(neg_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT WORKS !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
